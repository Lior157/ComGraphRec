{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rs_fixed_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwii302HJvGs"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8hpKrWHLunb"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ3B_udBIqV8"
      },
      "source": [
        "!pip install leidenalg\n",
        "!pip install cairocffi\n",
        "\n",
        "import leidenalg\n",
        "import igraph as ig\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "random.seed(1234)\n",
        "\n",
        "\n",
        "\n",
        "class arguments:\n",
        "    dataset = \"Ciao\"\n",
        "    test_prop = 0.1\n",
        "    enable_improvement = True\n",
        "\n",
        "args = arguments()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjVwmXusLBcA"
      },
      "source": [
        "# path to datasets\n",
        "workdir = '/content/gdrive/MyDrive/RS_prog/datasets/'\n",
        "\n",
        "# path for model check points saving\n",
        "workdir_checkpoints_save = \"/content/gdrive/MyDrive/RS_prog/checkpoints/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbo2apdUqY3l"
      },
      "source": [
        "# load dataset\n",
        "if args.dataset == 'Ciao':\n",
        "\tclick_f = loadmat(workdir + 'Ciao/rating.mat')['rating']\n",
        "\ttrust_f = loadmat(workdir + 'Ciao/trustnetwork.mat')['trustnetwork']\n",
        "elif args.dataset == 'Epinions':\n",
        "\tclick_f = np.loadtxt(workdir+'Epinions/ratings_data.txt', dtype = np.int32)\n",
        "\ttrust_f = np.loadtxt(workdir+'Epinions/trust_data.txt', dtype = np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSAxecuYqZxs"
      },
      "source": [
        "trust_f[456]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvyB7gWvJOEs"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toCgbEvUJR_S",
        "outputId": "7c349803-f1d0-4610-fc84-f1c7e469c51e"
      },
      "source": [
        "\n",
        "# load dataset\n",
        "if args.dataset == 'Ciao':\n",
        "\tclick_f = loadmat(workdir + 'Ciao/rating.mat')['rating']\n",
        "\ttrust_f = loadmat(workdir + 'Ciao/trustnetwork.mat')['trustnetwork']\n",
        "elif args.dataset == 'Epinions':\n",
        "\tclick_f = np.loadtxt(workdir+'Epinions/ratings_data.txt', dtype = np.int32)\n",
        "\ttrust_f = np.loadtxt(workdir+'Epinions/trust_data.txt', dtype = np.int32)\n",
        "else:\n",
        "\tpass \n",
        "\n",
        "click_list = []\n",
        "trust_list = []\n",
        "\n",
        "u_items_list = []\n",
        "u_users_list = []\n",
        "u_users_items_list = []\n",
        "i_users_list = []\n",
        "\n",
        "pos_u_items_list = []\n",
        "pos_i_users_list = []\n",
        "\n",
        "############### added for improvment\n",
        "u_users_items_list_group = []\n",
        "u_users_list_group = []\n",
        "###############\n",
        "\n",
        "user_count = 0\n",
        "item_count = 0\n",
        "rate_count = 0\n",
        "\n",
        "# read <user, item, rating> file, and create a list of arrays(each one represents a sample)\n",
        "for s in click_f:\n",
        "\tuid = s[0]\n",
        "\tiid = s[1]\n",
        "\tif args.dataset == 'Ciao':\n",
        "\t\tlabel = s[3]\n",
        "\telif args.dataset == 'Epinions':\n",
        "\t\tlabel = s[2]\n",
        "\n",
        "\tif uid > user_count:\n",
        "\t\tuser_count = uid\n",
        "\tif iid > item_count:\n",
        "\t\titem_count = iid\n",
        "\tif label > rate_count:\n",
        "\t\trate_count = label\n",
        "\tclick_list.append([uid, iid, label])\n",
        "\n",
        "\n",
        "pos_list = []\n",
        "for i in range(len(click_list)):\n",
        "\tpos_list.append((click_list[i][0], click_list[i][1], click_list[i][2]))\n",
        "\n",
        "# remove duplicate items in pos_list because there are some cases where a user may have different rate scores on the same item.\n",
        "pos_list = list(set(pos_list))\n",
        "\n",
        "# filter user less than 5 items\n",
        "pos_df = pd.DataFrame(pos_list, columns = ['uid', 'iid', 'label'])\n",
        "filter_pos_list = []\n",
        "user_in_set, user_out_set = set(), set()\n",
        "for u in range(user_count + 1):\n",
        "\thist = pos_df[pos_df['uid'] == u]\n",
        "\tif len(hist) < 5:\n",
        "\t\tuser_out_set.add(u)\n",
        "\t\tcontinue\n",
        "\tuser_in_set.add(u)\n",
        "\tu_items = hist['iid'].tolist()\n",
        "\tu_ratings = hist['label'].tolist()\n",
        "\tfilter_pos_list.extend([(u, iid, rating) for iid, rating in zip(u_items, u_ratings)])\n",
        "# print('user in and out size: ', len(user_in_set), len(user_out_set))\n",
        "\n",
        "\n",
        "db_user_before_filtering = len(pos_df['uid'].unique())\n",
        "print('No users before: ', db_user_before_filtering,' , after: ', len( user_in_set))\n",
        "print('No. Ratings before:', len(pos_list),' and after filtering: ', len(filter_pos_list))\n",
        "db_items_before_filtering = len(pos_df['uid'].unique())\n",
        "ratings_density_before = len(pos_list) / (db_user_before_filtering * db_items_before_filtering)\n",
        "\n",
        "\n",
        "# train, valid and test data split\n",
        "print('test prop: ', args.test_prop)\n",
        "pos_list = filter_pos_list\n",
        "random.shuffle(pos_list)\n",
        "num_test = int(len(pos_list) * args.test_prop)\n",
        "test_set = pos_list[:num_test]\n",
        "valid_set = pos_list[num_test:2 * num_test]\n",
        "train_set = pos_list[2 * num_test:]\n",
        "print('Train samples: {}, Valid samples: {}, Test samples: {}, Total samples: {}'.format(len(train_set), len(valid_set), len(test_set), len(pos_list)))\n",
        "\n",
        "# save splitted train, validaition, test sets to folder\n",
        "with open(workdir + args.dataset + '/dataset_filter5_'+str(args.test_prop)+'.pkl', 'wb') as f:\n",
        "\tpickle.dump(train_set, f, pickle.HIGHEST_PROTOCOL)\n",
        "\tpickle.dump(valid_set, f, pickle.HIGHEST_PROTOCOL)\n",
        "\tpickle.dump(test_set, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "# convert the splitted sets to DataFrames\n",
        "pos_df = pd.DataFrame(pos_list, columns = ['uid', 'iid', 'label'])\n",
        "train_df = pd.DataFrame(train_set, columns = ['uid', 'iid', 'label'])\n",
        "valid_df = pd.DataFrame(valid_set, columns = ['uid', 'iid', 'label'])\n",
        "test_df = pd.DataFrame(test_set, columns = ['uid', 'iid', 'label'])\n",
        "\n",
        "click_df = pd.DataFrame(click_list, columns = ['uid', 'iid', 'label'])\n",
        "\n",
        "# sort values by user_id\n",
        "train_df = train_df.sort_values(axis = 0, ascending = True, by = 'uid')\n",
        "pos_df = pos_df.sort_values(axis = 0, ascending = True, by = 'uid')\n",
        "\n",
        "\"\"\"\n",
        "creates list (u_items_list) of arrays. Each array contains tuples of (item id, rating), belongs to specific user. \n",
        "if user didn't have rating items the following tuple [(0, 0)] is added. \n",
        "\"\"\"\n",
        "for u in range(user_count + 1):\n",
        "\thist = train_df[train_df['uid'] == u]\n",
        "\tu_items = hist['iid'].tolist()\n",
        "\tu_ratings = hist['label'].tolist()\n",
        "\tif u_items == []:\n",
        "\t\tu_items_list.append([(0, 0)])\n",
        "\telse:\n",
        "\t\tu_items_list.append([(iid, rating) for iid, rating in zip(u_items, u_ratings)])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "creates list (i_users_list) of arrays. Each array contains tuples of (user id, rating), belongs to specific item. \n",
        "if item didn't rated by any user the following tuple [(0, 0)] is added. \n",
        "\"\"\"\n",
        "train_df = train_df.sort_values(axis = 0, ascending = True, by = 'iid')\n",
        "\n",
        "userful_item_set = set()\n",
        "for i in range(item_count + 1):\n",
        "\thist = train_df[train_df['iid'] == i]\n",
        "\ti_users = hist['uid'].tolist()\n",
        "\ti_ratings = hist['label'].tolist()\n",
        "\tif i_users == []:\n",
        "\t\ti_users_list.append([(0, 0)])\n",
        "\telse:\n",
        "\t\ti_users_list.append([(uid, rating) for uid, rating in zip(i_users, i_ratings)])\n",
        "\t\tuserful_item_set.add(i)\n",
        "\n",
        "print('item size before: ', item_count,\"  and after filtering: \", len(userful_item_set))\n",
        "ratings_density_after = len(pos_list)/(len(user_in_set)*len(userful_item_set))\n",
        "\n",
        "\n",
        "print('Ratings Density before: ', ratings_density_before , ' , after: ', ratings_density_after)\n",
        "\n",
        "# save postprocessed user-item-rating data files\n",
        "with open(workdir + args.dataset + '/effective_users_and items_filter5.pkl', 'wb') as f:\n",
        "\tpickle.dump(list(user_in_set), f, pickle.HIGHEST_PROTOCOL)\n",
        "\tpickle.dump(list(userful_item_set), f, pickle.HIGHEST_PROTOCOL)\n",
        " \n",
        "\"\"\"\n",
        "processing of social relations file, reads data to list that represents egdes\n",
        "of user nodes connections, [A,B] means user A follow after user B. \n",
        " \"\"\"\n",
        "count_f_origin, count_f_filter = 0,0\n",
        "for s in trust_f:\n",
        "\tuid = s[0]\n",
        "\tfid = s[1]\n",
        "\tcount_f_origin += 1\n",
        "\tif uid > user_count or fid > user_count:\n",
        "\t\tcontinue\n",
        "\tif uid in user_out_set or fid in user_out_set:\n",
        "\t\tcontinue\n",
        "\ttrust_list.append([uid, fid])\n",
        "\tcount_f_filter += 1\n",
        "\n",
        "print('No. of social connections before: ' , count_f_origin, ' ,after: ', count_f_filter)\n",
        "social_density_after = count_f_filter/(len(user_in_set)**2)\n",
        "social_density_before = count_f_origin/(db_user_before_filtering**2)\n",
        "print('social density before: ',   social_density_before , ', after:', social_density_after)\n",
        "trust_df = pd.DataFrame(trust_list, columns = ['uid', 'fid'])\n",
        "trust_df = trust_df.sort_values(axis = 0, ascending = True, by = 'uid')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "u_users_list: list of users contains for each one aggregation of users that he follows, if he didnt follow any user [0] added.  \n",
        "u_users_items_list: list of users contains for each one list of list represents items and rating of following users.\n",
        "if user didnt follow any user [0,0] added.\n",
        "\"\"\"\n",
        "count_0, count_1 = 0,0\n",
        "for u in range(user_count + 1):\n",
        "\thist = trust_df[trust_df['uid'] == u]\n",
        "\tu_users = hist['fid'].unique().tolist()\n",
        "\tif u_users == []:\n",
        "\t\tu_users_list.append([0])\n",
        "\t\tu_users_items_list.append([[(0,0)]])\n",
        "\t\tcount_0 += 1\n",
        "\telse:\n",
        "\t\tu_users_list.append(u_users)\n",
        "\t\tuu_items = []\n",
        "\t\tfor uid in u_users:\n",
        "\t\t\tuu_items.append(u_items_list[uid])\n",
        "\t\tu_users_items_list.append(uu_items)\n",
        "\t\tcount_1 += 1\n",
        "\n",
        "print('trust user with items size: ', count_0, count_1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No users before:  40163  , after:  23253\n",
            "No. Ratings before: 664824  and after filtering:  631192\n",
            "test prop:  0.1\n",
            "Train samples: 504954, Valid samples: 63119, Test samples: 63119, Total samples: 631192\n",
            "item size before:  139738   and after filtering:  120711\n",
            "Ratings Density before:  0.00041214914051076584  , after:  0.00022487213666856555\n",
            "No. of social connections before:  487183  ,after:  374039\n",
            "social density before:  0.0003020228733039969 , after: 0.0006917655081248274\n",
            "trust user with items size:  31405 17885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhSZmTRZsUxx"
      },
      "source": [
        "## **Communities detection - Leidenalg**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HOeX_4CskoT"
      },
      "source": [
        "\n",
        "# load social graph egdes\n",
        "tuples = [tuple(x) for x in trust_df.values]\n",
        "g3 = ig.Graph.TupleList(tuples, directed = True)\n",
        "\n",
        "# partition of social graph to communities \n",
        "part = leidenalg.find_partition(g3, leidenalg.SignificanceVertexPartition, n_iterations=-1, seed=42, max_comm_size=200)\n",
        "\n",
        "com_list = [com for com in part if len(com) > 2]\n",
        "with open('/content/gdrive/MyDrive/RS_prog/comm_list_'+args.dataset+'.pkl', 'wb') as fp:\n",
        "    pickle.dump(com_list, fp)\n",
        "\n",
        "# read communities file, after prepare using a communities detection algorithm\n",
        "with open ('/content/gdrive/MyDrive/RS_prog/comm_list_'+args.dataset+'.pkl', 'rb') as fp:\n",
        "    comm_list = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGqNInJ0-Yi"
      },
      "source": [
        "## **Preprocessing Improvement** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t0XGM-PznS9"
      },
      "source": [
        "# improvement group list ###################################################\n",
        "dict_users_groups = {}\n",
        "\n",
        "# create for each user list of his community users \n",
        "for scc in comm_list:\n",
        "    for person in scc:\n",
        "      list_scc = scc.copy()\n",
        "      list_scc.remove(person)\n",
        "      dict_users_groups[person] = list_scc\n",
        "\n",
        "\"\"\"\n",
        "preparing data of communities\n",
        "u_users_list_group: list of users contains for each one aggregation of users that part of his community, if he doesnt part of community [0] added.  \n",
        "u_users_items_list_group: list of users contains for each one list of list represents items and rating of his community users.\n",
        "if user didnt part of any community [0,0] added.\n",
        "\"\"\"\n",
        "for u in range(user_count + 1):\n",
        "    u_users = []\n",
        "    if u in dict_users_groups:\n",
        "\n",
        "        u_users = dict_users_groups[u]\n",
        "\n",
        "    if u_users == []:\n",
        "        u_users_list_group.append([0])\n",
        "        u_users_items_list_group.append([[(0,0)]])\n",
        "    else:\n",
        "        u_users_list_group.append(u_users)\n",
        "        uu_items = []\n",
        "        for uid in u_users:\n",
        "          uu_items.append(u_items_list[uid])\n",
        "        u_users_items_list_group.append(uu_items)\n",
        "\n",
        "######################### end preprocessing - improvment part\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OThiRXxb6l6p"
      },
      "source": [
        "## **Save processed data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8ul5W-x6kcz"
      },
      "source": [
        "# save preprocessing data\n",
        "with open(workdir + args.dataset + '/list_filter5_'+str(args.test_prop)+'.pkl', 'wb') as f:\n",
        "      pickle.dump(u_items_list, f, pickle.HIGHEST_PROTOCOL)\n",
        "      pickle.dump(u_users_list, f, pickle.HIGHEST_PROTOCOL)\n",
        "      pickle.dump(u_users_items_list, f, pickle.HIGHEST_PROTOCOL)\n",
        "      pickle.dump(i_users_list, f, pickle.HIGHEST_PROTOCOL)\n",
        "      pickle.dump((user_count, item_count, rate_count), f, pickle.HIGHEST_PROTOCOL)\n",
        "      pickle.dump(u_users_list_group, f, pickle.HIGHEST_PROTOCOL)        \n",
        "      pickle.dump(u_users_items_list_group, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb45cyu3JEar"
      },
      "source": [
        "# **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WNJ82EJGYq"
      },
      "source": [
        "\n",
        "truncate_len = 50\n",
        "truncate_len_i = 10\n",
        "\n",
        "\"\"\"\n",
        "Ciao dataset info:\n",
        "Avg number of items rated per user: 38.3\n",
        "Avg number of users interacted per user: 2.7\n",
        "Avg number of users connected per item: 16.4\n",
        "\"\"\"\n",
        "\n",
        "def collate_fn(batch_data):\n",
        "    \"\"\"\n",
        "    This function will be used to pad the graph to max length in the batch.\n",
        "    It will be used in the Dataloader.\n",
        "    \"\"\"\n",
        "    uids, iids, labels = [], [], []\n",
        "    u_items, u_users, u_users_items, i_users, u_users_group, u_users_items_group  = [], [], [], [], [], []\n",
        "    u_items_len, u_users_len, i_users_len, u_users_len_group = [], [], [], []\n",
        "    count = 0\n",
        "    for data, u_items_u, u_users_u, u_users_items_u, i_users_i, u_users_u_group, u_users_items_u_group in batch_data:\n",
        "\n",
        "        (uid, iid, label) = data\n",
        "        uids.append(uid)\n",
        "        iids.append(iid)\n",
        "        labels.append(label)\n",
        "\n",
        "        # user-items    \n",
        "        if len(u_items_u) <= truncate_len:\n",
        "            u_items.append(u_items_u)\n",
        "        else:\n",
        "            u_items.append(random.sample(u_items_u, truncate_len))\n",
        "        u_items_len.append(min(len(u_items_u), truncate_len))\n",
        "\n",
        "\n",
        "        # user-users and user-users-items\n",
        "        if len(u_users_u) < truncate_len:\n",
        "            tmp_users = [item for item in u_users_u]\n",
        "            tmp_users.append(uid)\n",
        "            u_users.append(tmp_users)\n",
        "            u_u_items = [] \n",
        "            for uui in u_users_items_u:\n",
        "                if len(uui) < truncate_len:\n",
        "                    u_u_items.append(uui)\n",
        "                else:\n",
        "                    u_u_items.append(random.sample(uui, truncate_len))\n",
        "            # self -loop\n",
        "            u_u_items.append(u_items[-1])\n",
        "            u_users_items.append(u_u_items)\n",
        "\n",
        "        else:\n",
        "            sample_index = random.sample(list(range(len(u_users_u))), truncate_len-1)\n",
        "            tmp_users = [u_users_u[si] for si in sample_index]\n",
        "            tmp_users.append(uid)\n",
        "            u_users.append(tmp_users)\n",
        "\n",
        "            u_users_items_u_tr = [u_users_items_u[si] for si in sample_index]\n",
        "            u_u_items = [] \n",
        "            for uui in u_users_items_u_tr:\n",
        "                if len(uui) < truncate_len:\n",
        "                    u_u_items.append(uui)\n",
        "                else:\n",
        "                    u_u_items.append(random.sample(uui, truncate_len))\n",
        "            u_u_items.append(u_items[-1])\n",
        "            u_users_items.append(u_u_items)\n",
        "\n",
        "        u_users_len.append(min(len(u_users_u)+1, truncate_len))\n",
        "\n",
        "        # item-users\n",
        "        if len(i_users_i) <= truncate_len:\n",
        "            i_users.append(i_users_i)\n",
        "        else:\n",
        "            i_users.append(random.sample(i_users_i, truncate_len))\n",
        "        i_users_len.append(min(len(i_users_i), truncate_len))\n",
        "\n",
        "################################################################################# added for improvement\n",
        "\n",
        "         # user-users group and user-users-items group\n",
        "        if len(u_users_u_group) <= truncate_len:\n",
        "            u_users_group.append(u_users_u_group)\n",
        "            u_u_items_group = [] \n",
        "            for uui_group in u_users_items_u_group:\n",
        "                if len(uui_group) < truncate_len:\n",
        "                    u_u_items_group.append(uui_group)\n",
        "                else:\n",
        "                    u_u_items_group.append(random.sample(uui_group, truncate_len))\n",
        "            u_users_items_group.append(u_u_items_group)\n",
        "        else:\n",
        "            sample_index = random.sample(list(range(len(u_users_u_group))), truncate_len)\n",
        "            u_users_group.append([u_users_u_group[si] for si in sample_index])\n",
        "\n",
        "            u_users_items_u_tr_group = [u_users_items_u_group[si] for si in sample_index]\n",
        "            u_u_items_group = [] \n",
        "            for uui_group in u_users_items_u_tr_group:\n",
        "                if len(uui_group) < truncate_len:\n",
        "                    u_u_items_group.append(uui_group)\n",
        "                else:\n",
        "                    u_u_items_group.append(random.sample(uui_group, truncate_len))\n",
        "            u_users_items_group.append(u_u_items_group)\n",
        "\n",
        "        u_users_len_group.append(truncate_len)\t #(min(len(u_users_u_group), truncate_len))\t\n",
        "\n",
        "################################################################################## until here\n",
        "    batch_size = len(batch_data)\n",
        "\n",
        "    # padding size\n",
        "    u_items_maxlen = max(u_items_len)\n",
        "    u_users_maxlen = max(u_users_len)\n",
        "    i_users_maxlen = max(i_users_len)\n",
        "\n",
        "    \n",
        "    u_item_pad = torch.zeros([batch_size, u_items_maxlen, 2], dtype=torch.long)\n",
        "    for i, ui in enumerate(u_items):\n",
        "        u_item_pad[i, :len(ui), :] = torch.LongTensor(ui)\n",
        "    \n",
        "    u_user_pad = torch.zeros([batch_size, u_users_maxlen], dtype=torch.long)\n",
        "    for i, uu in enumerate(u_users):\n",
        "        u_user_pad[i, :len(uu)] = torch.LongTensor(uu)\n",
        "\n",
        "\n",
        "    u_user_item_pad = torch.zeros([batch_size, u_users_maxlen, u_items_maxlen, 2], dtype=torch.long)\n",
        "    for i, uu_items in enumerate(u_users_items):\n",
        "        for j, ui in enumerate(uu_items):\n",
        "            u_user_item_pad[i, j, :len(ui), :] = torch.LongTensor(ui)\n",
        "\n",
        "    i_user_pad = torch.zeros([batch_size, i_users_maxlen, 2], dtype=torch.long)\n",
        "    for i, iu in enumerate(i_users):\n",
        "        i_user_pad[i, :len(iu), :] = torch.LongTensor(iu)\n",
        "\n",
        "################## added for improvement\n",
        "    u_users_maxlen_group = max(u_users_len_group)\n",
        "\n",
        "    u_user_pad_group = torch.zeros([batch_size, u_users_maxlen_group], dtype=torch.long)\n",
        "    for i, uu in enumerate(u_users_group):\n",
        "        u_user_pad_group[i, :len(uu)] = torch.LongTensor(uu)\n",
        "\n",
        "    u_user_item_pad_group = torch.zeros([batch_size, u_users_maxlen_group, u_items_maxlen, 2], dtype=torch.long)\n",
        "    for i, uu_items in enumerate(u_users_items_group):\n",
        "        for j, ui in enumerate(uu_items):\n",
        "            u_user_item_pad_group[i, j, :len(ui), :] = torch.LongTensor(ui)\n",
        "####################\n",
        "\n",
        "    return torch.LongTensor(uids), torch.LongTensor(iids), torch.FloatTensor(labels), \\\n",
        "            u_item_pad, u_user_pad, u_user_item_pad, i_user_pad, u_user_pad_group, u_user_item_pad_group"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXmxmy3IIqyn"
      },
      "source": [
        "# **Data Loader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLEypulUIniR"
      },
      "source": [
        "class GRDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This class provides an envelope for iterating over the data.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, u_items_list, u_users_list, u_users_items_list, i_users_list, u_users_list_group, u_users_items_list_group):\n",
        "          \"\"\"\n",
        "          Initializing the class instance.\n",
        "          \"\"\"\n",
        "          self.data = data\n",
        "          self.u_items_list = u_items_list\n",
        "          self.u_users_list = u_users_list\n",
        "          self.u_users_items_list = u_users_items_list\n",
        "          self.i_users_list = i_users_list\n",
        "          self.u_users_list_group = u_users_list_group\n",
        "          self.u_users_items_list_group = u_users_items_list_group\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "          \"\"\"\n",
        "          get all graph info relevent for data in given index. \n",
        "          \"\"\"\n",
        "          uid = self.data[index][0]\n",
        "          iid = self.data[index][1]\n",
        "          label = self.data[index][2]\n",
        "          u_items = self.u_items_list[uid]\n",
        "          u_users = self.u_users_list[uid]\n",
        "          u_users_items = self.u_users_items_list[uid]\n",
        "          i_users = self.i_users_list[iid]\n",
        "\n",
        "          ###################\n",
        "          u_users_group = self.u_users_list_group[uid]\n",
        "          u_users_items_group = self.u_users_items_list_group[uid]\n",
        "          ###################\n",
        "\n",
        "          return (uid, iid, label), u_items, u_users, u_users_items, i_users, u_users_group, u_users_items_group\n",
        "\n",
        "    def __len__(self):\n",
        "          \"\"\"\n",
        "          get data length.\n",
        "          \"\"\"\n",
        "          return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkfn8OSPI2eW"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nVR_q3oI4dD"
      },
      "source": [
        "class _MultiLayerPercep(nn.Module):\n",
        "    \"\"\"\n",
        "    Auxiliary class of MLP functionality.\n",
        "    This structure called from GraphRec structure.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Initializition of MLP structure.\n",
        "\n",
        "        Parameters: \n",
        "        input_dim - input size for MLP network\n",
        "        output_dim - output size for MLP network\n",
        "        \"\"\"\n",
        "        super(_MultiLayerPercep, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim // 2, bias=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_dim // 2, output_dim, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        MLP forward functionality\n",
        "        \"\"\"\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "class _Aggregation(nn.Module):\n",
        "    \"\"\"\n",
        "    Auxiliary class for items/users aggregation.\n",
        "    This structure called from GraphRec structure.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Initializition of aggregation structure.\n",
        "\n",
        "        Parameters: \n",
        "        input_dim - input size for neural network\n",
        "        output_dim - output size for neural network\n",
        "\n",
        "        \"\"\"\n",
        "        super(_Aggregation, self).__init__()\n",
        "        self.aggre = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim, bias=True),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Neural network forward functionality\n",
        "        \"\"\"\n",
        "        return self.aggre(x)\n",
        "\n",
        "\n",
        "class _UserModel(nn.Module):\n",
        "    ''' \n",
        "    User modeling to learn user latent factors.\n",
        "    User modeling leverages two types aggregation: item aggregation and social aggregation\n",
        "\n",
        "    social aggregation built from two information sets:\n",
        "    1. trust graph (how each user follow)\n",
        "    2. communities graph (community information)\n",
        "    '''\n",
        "    def __init__(self, emb_dim, user_emb, item_emb, rate_emb):\n",
        "\n",
        "        super(_UserModel, self).__init__()\n",
        "        self.user_emb = user_emb\n",
        "        self.item_emb = item_emb\n",
        "        self.rate_emb = rate_emb\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.w1 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w2 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w3 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w4 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w5 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w6 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w7 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        #### added\n",
        "\n",
        "        self.w8 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w9 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.g_v_social = _MultiLayerPercep(2 * self.emb_dim, self.emb_dim)\n",
        "        ####\n",
        "\n",
        "\n",
        "        self.g_v = _MultiLayerPercep(2 * self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.user_items_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_items = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.user_items_att_s1 = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_items_s1 = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "        self.user_users_att_s2 = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_neigbors_s2 = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.u_user_users_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.u_aggre_neigbors = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        #### added for improvement\n",
        "        self.user_users_att_group = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_neigbors_group = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "\n",
        "        self.combine_mlp_social = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(2 * self.emb_dim, 2*self.emb_dim, bias = True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(2*self.emb_dim, self.emb_dim, bias = True),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        ####\n",
        "\n",
        "        self.combine_mlp = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(2 * self.emb_dim, 2*self.emb_dim, bias = True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(2*self.emb_dim, self.emb_dim, bias = True),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # used for preventing zero div error when calculating softmax score\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def forward(self, uids, u_item_pad, u_user_pad, u_user_item_pad, u_user_pad_group, u_user_item_pad_group):\n",
        "        \"\"\"\n",
        "        User modeling part, forward information pass.\n",
        "        Returns user latent space. \n",
        "        \"\"\"\n",
        "        # item aggregation\n",
        "        q_a = self.item_emb(u_item_pad[:,:,0])   # B x maxi_len x emb_dim\n",
        "        mask_u = torch.where(u_item_pad[:,:,0] > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))   # B x maxi_len\n",
        "        u_item_er = self.rate_emb(u_item_pad[:, :, 1])  # B x maxi_len x emb_dim\n",
        "        x_ia = self.g_v(torch.cat([q_a, u_item_er], dim=2).view(-1, 2 * self.emb_dim)).view(q_a.size())  # B x maxi_len x emb_dim\n",
        "        p_i = mask_u.unsqueeze(2).expand_as(q_a) * self.user_emb(uids).unsqueeze(1).expand_as(q_a)  # B x maxi_len x emb_dim\n",
        "\n",
        "        alpha = self.user_items_att(torch.cat([self.w1(x_ia), self.w1(p_i)], dim = 2).view(-1, 2 * self.emb_dim)).view(mask_u.size()) # B x maxi_len\n",
        "        alpha = torch.exp(alpha) * mask_u\n",
        "        alpha = alpha / (torch.sum(alpha, 1).unsqueeze(1).expand_as(alpha) + self.eps)\n",
        "\n",
        "        h_iI = self.aggre_items(torch.sum(alpha.unsqueeze(2).expand_as(x_ia) * x_ia, 1))     # B x emb_dim\n",
        "        h_iI = F.dropout(h_iI, 0.5, training=self.training)\n",
        "\n",
        "\n",
        "\n",
        "        # social aggregation\n",
        "        q_a_s = self.item_emb(u_user_item_pad[:,:,:,0])   # B x maxu_len x maxi_len x emb_dim\n",
        "        mask_s = torch.where(u_user_item_pad[:,:,:,0] > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))  # B x maxu_len x maxi_len\n",
        "        p_i_s = mask_s.unsqueeze(3).expand_as(q_a_s) * self.user_emb(u_user_pad).unsqueeze(2).expand_as(q_a_s)  # B x maxu_len x maxi_len x emb_dim\n",
        "        u_user_item_er = self.rate_emb(u_user_item_pad[:, :, :, 1])  # B x maxu_len x maxi_len x emb_dim\n",
        "        x_ia_s = self.g_v(torch.cat([q_a_s, u_user_item_er], dim=3).view(-1, 2 * self.emb_dim)).view(q_a_s.size())  # B x maxu_len x maxi_len x emb_dim\n",
        "\n",
        "        alpha_s = self.user_items_att_s1(torch.cat([self.w4(x_ia_s), self.w4(p_i_s)], dim = 3).view(-1, 2 * self.emb_dim)).view(mask_s.size())    # B x maxu_len x maxi_len\n",
        "        alpha_s = torch.exp(alpha_s) * mask_s\n",
        "        alpha_s = alpha_s / (torch.sum(alpha_s, 2).unsqueeze(2).expand_as(alpha_s) + self.eps)\n",
        "\n",
        "\n",
        "        h_oI_temp = torch.sum(alpha_s.unsqueeze(3).expand_as(x_ia_s) * x_ia_s, 2)    # B x maxu_len x emb_dim\n",
        "        h_oI = self.aggre_items_s1(h_oI_temp.view(-1, self.emb_dim)).view(h_oI_temp.size())  # B x maxu_len x emb_dim\n",
        "        h_oI = F.dropout(h_oI, p=0.5, training=self.training)\n",
        "\n",
        "        ## calculate attention scores in social aggregation\n",
        "        mask_su = torch.where(u_user_pad > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
        "\n",
        "        beta = self.user_users_att_s2(torch.cat([self.w5(h_oI), self.w5(self.user_emb(u_user_pad))], dim = 2).view(-1, 2 * self.emb_dim)).view(u_user_pad.size())\n",
        "        beta = torch.exp(beta) * mask_su\n",
        "        beta = beta / (torch.sum(beta, 1).unsqueeze(1).expand_as(beta) + self.eps)\n",
        "        h_iS = self.aggre_neigbors_s2(torch.sum(beta.unsqueeze(2).expand_as(h_oI) * h_oI, 1))     # B x emb_dim\n",
        "        h_iS = F.dropout(h_iS, p=0.5, training=self.training)\n",
        "\n",
        "        if args.enable_improvement:\n",
        "            # group social aggregation\n",
        "            q_a_s_group = self.item_emb(u_user_item_pad_group[:,:,:,0])   # B x maxu_len x maxi_len x emb_dim\n",
        "            mask_s_group = torch.where(u_user_item_pad_group[:,:,:,0] > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))  # B x community_maxu_len x maxi_len\n",
        "            p_i_s_group = mask_s_group.unsqueeze(3).expand_as(q_a_s_group) * self.user_emb(u_user_pad_group).unsqueeze(2).expand_as(q_a_s_group)  # B x community_maxu_len x maxi_len x emb_dim\n",
        "            u_user_item_er_group = self.rate_emb(u_user_item_pad_group[:,:,:,1]) # B x maxu_len x maxi_len x emb_dim\n",
        "            x_ia_s_group = self.g_v_social(torch.cat([q_a_s_group, u_user_item_er_group], dim = 3).view(-1, 2 * self.emb_dim)).view(q_a_s_group.size())  # B x community_maxu_len x maxi_len x emb_dim   \n",
        "\n",
        "\n",
        "            alpha_s_group = self.user_items_att(torch.cat([self.w8(x_ia_s_group), self.w8(p_i_s_group)], dim = 3).view(-1, 2 * self.emb_dim)).view(mask_s_group.size())    # B x community_maxu_len x maxi_len\n",
        "            alpha_s_group = torch.exp(alpha_s_group) * mask_s_group\n",
        "            alpha_s_group = alpha_s_group / (torch.sum(alpha_s_group, 2).unsqueeze(2).expand_as(alpha_s_group) + self.eps)\n",
        "\n",
        "            h_oI_temp_group = torch.sum(alpha_s_group.unsqueeze(3).expand_as(x_ia_s_group) * x_ia_s_group, 2)    # B x community_maxu_len x emb_dim\n",
        "            h_oI_group = self.aggre_items(h_oI_temp_group.view(-1, self.emb_dim)).view(h_oI_temp_group.size())  # B x community_maxu_len x emb_dim\n",
        "\n",
        "            mask_su_group = torch.where(u_user_pad_group > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
        "\n",
        "            ## group - calculate attention scores in social aggregation\n",
        "            beta_group = self.user_users_att_group(torch.cat([self.w9(h_oI_group), self.w9(self.user_emb(u_user_pad_group))], dim = 2).view(-1, 2 * self.emb_dim)).view(u_user_pad_group.size())\n",
        "            beta_group = torch.exp(beta_group) * mask_su_group\n",
        "            beta_group = beta_group / (torch.sum(beta_group, 1).unsqueeze(1).expand_as(beta_group) + self.eps)\n",
        "            h_iS_group = self.aggre_neigbors_group(torch.sum(beta.unsqueeze(2).expand_as(h_oI_group) * h_oI_group, 1))     # B x emb_dim\n",
        "            h_iS_group = F.dropout(h_iS_group, p=0.5, training=self.training)\n",
        "\n",
        "\n",
        "            ##  combine community and trust(friends) spaces\n",
        "            h_iS = self.combine_mlp_social(torch.cat([h_iS_group, h_iS], dim = 1))\n",
        "\n",
        "\n",
        "        ## learning user latent factor\n",
        "        h =  self.combine_mlp(torch.cat([h_iI, h_iS], dim = 1))\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class _ItemModel(nn.Module):\n",
        "    '''Item modeling to learn item latent factors.\n",
        "    '''\n",
        "    def __init__(self, emb_dim, user_emb, item_emb, rate_emb):\n",
        "        super(_ItemModel, self).__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.user_emb = user_emb\n",
        "        self.item_emb = item_emb\n",
        "        self.rate_emb = rate_emb\n",
        "\n",
        "        self.w1 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w2 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w3 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "        self.w4 = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.g_u = _MultiLayerPercep(2 * self.emb_dim, self.emb_dim)\n",
        "        self.g_v = _MultiLayerPercep(2 * self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.item_users_att_i = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_users_i = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.item_users_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_users = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.i_friends_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_i_friends = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.if_friends_att = _MultiLayerPercep(2 * self.emb_dim, 1)\n",
        "        self.aggre_if_friends = _Aggregation(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.combine_mlp = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(3* self.emb_dim, 2*self.emb_dim, bias = True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(2*self.emb_dim, self.emb_dim, bias = True),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # used for preventing zero div error when calculating softmax score\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def forward(self, iids, i_user_pad):\n",
        "        \"\"\"\n",
        "        Item modeling part, forward information pass.\n",
        "        Returns item latent space. \n",
        "        \"\"\"\n",
        "        # user aggregation\n",
        "        p_t = self.user_emb(i_user_pad[:,:,0])\n",
        "        mask_i = torch.where(i_user_pad[:,:,0] > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
        "        i_user_er = self.rate_emb(i_user_pad[:,:,1])\n",
        "        f_jt = self.g_u(torch.cat([p_t, i_user_er], dim = 2).view(-1, 2 * self.emb_dim)).view(p_t.size())\n",
        "        \n",
        "        # calculate attention scores in user aggregation\n",
        "        q_j = mask_i.unsqueeze(2).expand_as(f_jt) * self.item_emb(iids).unsqueeze(1).expand_as(f_jt)\n",
        "        \n",
        "        miu = self.item_users_att_i(torch.cat([self.w1(f_jt), self.w1(q_j)], dim = 2).view(-1, 2 * self.emb_dim)).view(mask_i.size())\n",
        "        miu = torch.exp(miu) * mask_i\n",
        "        miu = miu / (torch.sum(miu, 1).unsqueeze(1).expand_as(miu) + self.eps)\n",
        "        z_j = self.aggre_users_i(torch.sum(miu.unsqueeze(2).expand_as(f_jt) * self.w1(f_jt), 1))\n",
        "        z_j = F.dropout(z_j, p=0.5, training=self.training)\n",
        "\n",
        "\n",
        "        return z_j\n",
        "\n",
        "\n",
        "class GraphRec(nn.Module):\n",
        "    '''\n",
        "    GraphRec model proposed in the paper Graph neural network for social recommendation \n",
        "    Args:\n",
        "        number_users: the number of users in the dataset.\n",
        "        number_items: the number of items in the dataset.\n",
        "        num_rate_levels: the number of rate levels in the dataset.\n",
        "        emb_dim: the dimension of user and item embedding (default = 64).\n",
        "    '''\n",
        "    def __init__(self, num_users, num_items, num_rate_levels, emb_dim = 64):\n",
        "        super(GraphRec, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.num_rate_levels = num_rate_levels\n",
        "        self.emb_dim = emb_dim\n",
        "        self.user_emb = nn.Embedding(self.num_users, self.emb_dim, padding_idx = 0)\n",
        "        self.item_emb = nn.Embedding(self.num_items, self.emb_dim, padding_idx = 0)\n",
        "        self.rate_emb = nn.Embedding(self.num_rate_levels, self.emb_dim, padding_idx = 0)\n",
        "\n",
        "        self.user_model = _UserModel(self.emb_dim, self.user_emb, self.item_emb, self.rate_emb)\n",
        "\n",
        "        self.item_model = _ItemModel(self.emb_dim, self.user_emb, self.item_emb, self.rate_emb)\n",
        "\n",
        "        \n",
        "        self.rate_pred = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(2* self.emb_dim, self.emb_dim, bias = True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(self.emb_dim, self.emb_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(self.emb_dim // 4, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, uids, iids, u_item_pad, u_user_pad, u_user_item_pad, i_user_pad, u_user_pad_group, u_user_item_pad_group):\n",
        "        '''\n",
        "        Args:\n",
        "            uids: the user id sequences.\n",
        "            iids: the item id sequences.\n",
        "            u_item_pad: the padded user-item graph.\n",
        "            u_user_pad: the padded user-user graph.\n",
        "            u_user_item_pad: the padded user-user-item graph.\n",
        "            i_user_pad: the padded item-user graph.\n",
        "            u_user_pad_group:the padded user-community(user) graph.\n",
        "            u_user_item_pad_group:the padded user-community(user)-item graph.\n",
        "        Shapes:\n",
        "            uids: (B).\n",
        "            iids: (B).\n",
        "            u_item_pad: (B, ItemSeqMaxLen, 2).\n",
        "            u_user_pad: (B, UserSeqMaxLen).\n",
        "            u_user_item_pad: (B, UserSeqMaxLen, ItemSeqMaxLen, 2).\n",
        "            i_user_pad: (B, UserSeqMaxLen, 2).\n",
        "            u_user_pad_group: (B, CommunityUserSeqMaxLen).\n",
        "            u_user_item_pad_group: (B, CommunityUserSeqMaxLen, ItemSeqMaxLen, 2).\n",
        "        Returns:\n",
        "            the predicted rate scores of the user to the item.\n",
        "        '''\n",
        "\n",
        "        h = self.user_model(uids, u_item_pad, u_user_pad, u_user_item_pad, u_user_pad_group, u_user_item_pad_group)\n",
        "        z = self.item_model(iids, i_user_pad)\n",
        "\n",
        "        r_ij = self.rate_pred(torch.cat([h,z], dim = 1))\n",
        "\n",
        "        return r_ij\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNjvWC3BJkQU"
      },
      "source": [
        "# **Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psZz-yfAJl6c"
      },
      "source": [
        "# run parameters\n",
        "dataset_path = workdir + args.dataset +\"/\"\n",
        "batch_size = 128\n",
        "embed_dim = 64\n",
        "args_epoch = 30\n",
        "seed = 1234\n",
        "lr = 0.001\n",
        "lr_dc = 0.5\n",
        "lr_dc_step = 30\n",
        "\n",
        "\n",
        "\n",
        "# set seed and run configuration: cpu/gpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "# set location for training weights checkpoints\n",
        "fn = workdir_checkpoints_save \n",
        "if not os.path.exists(fn):\n",
        "    os.mkdir(fn)\n",
        "\n",
        "fn = workdir_checkpoints_save + args.dataset +'_'+str(args.test_prop)+\"/\" \n",
        "if not os.path.exists(fn):\n",
        "    os.mkdir(fn)\n",
        "\n",
        "\n",
        "# main function for train/test \n",
        "def main(test =  False ):\n",
        "\n",
        "    \n",
        "    # load train, validation, test sets\n",
        "    print('Loading data...')\n",
        "    with open(dataset_path + 'dataset_filter5_'+str(args.test_prop)+'.pkl', 'rb') as f:\n",
        "        train_set = pickle.load(f)\n",
        "        valid_set = pickle.load(f)\n",
        "        test_set = pickle.load(f)\n",
        "\n",
        "\n",
        "    # load preprocessed information for graphs\n",
        "    with open(dataset_path + 'list_filter5_'+str(args.test_prop)+'.pkl', 'rb') as f:\n",
        "        u_items_list = pickle.load(f)\n",
        "        u_users_list = pickle.load(f)\n",
        "        u_users_items_list = pickle.load(f)\n",
        "        i_users_list = pickle.load(f)\n",
        "        (user_count, item_count, rate_count) = pickle.load(f)\n",
        "        u_users_list_group = pickle.load(f)\n",
        "        u_users_items_list_group = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "    # prepare data as torch DataLoader \n",
        "    train_data = GRDataset(train_set, u_items_list,  u_users_list, u_users_items_list, i_users_list,u_users_list_group,u_users_items_list_group)\n",
        "    valid_data = GRDataset(valid_set, u_items_list,  u_users_list, u_users_items_list, i_users_list,u_users_list_group,u_users_items_list_group)\n",
        "    test_data = GRDataset(test_set, u_items_list, u_users_list,  u_users_items_list, i_users_list,u_users_list_group,u_users_items_list_group)\n",
        "    train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, collate_fn = collate_fn)\n",
        "    valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = False, collate_fn = collate_fn)\n",
        "    test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False, collate_fn = collate_fn)\n",
        "\n",
        "\n",
        "    model = GraphRec(user_count+1, item_count+1, rate_count+1, embed_dim).to(device)\n",
        "\n",
        "    # if it's test run, run model on test data, and print rmse and mae score.\n",
        "    if test:\n",
        "        print('Load checkpoint and testing...')\n",
        "        ckpt = torch.load('%s/random_best_checkpoint.pth.tar' %fn)\n",
        "        model.load_state_dict(ckpt['state_dict'])\n",
        "        mae, rmse = validate(test_loader, model)\n",
        "        print(\"Test: MAE: {:.4f}, RMSE: {:.4f}\".format(mae, rmse))\n",
        "        return\n",
        "\n",
        "def trainForEpoch(train_loader, model, optimizer, epoch, num_epochs, criterion, log_aggr=1):\n",
        "    model.train()\n",
        "\n",
        "    sum_epoch_loss = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for i, (uids, iids, labels, u_items, u_users, u_users_items, i_users,u_user_group, u_user_item_group) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "        uids = uids.to(device)\n",
        "        iids = iids.to(device)\n",
        "        labels = labels.to(device)\n",
        "        u_items = u_items.to(device)\n",
        "        u_users = u_users.to(device)\n",
        "        u_users_items = u_users_items.to(device)\n",
        "        i_users = i_users.to(device)\n",
        "        u_user_group =u_user_group.to(device)#### added \n",
        "        u_user_item_group = u_user_item_group.to(device)#### added \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(uids, iids, u_items, u_users, u_users_items, i_users, u_user_group, u_user_item_group)#### changed\n",
        "\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "\n",
        "        loss_val = loss.item()\n",
        "        sum_epoch_loss += loss_val\n",
        "\n",
        "        iter_num = epoch * len(train_loader) + i + 1\n",
        "\n",
        "        if i % log_aggr == 0:\n",
        "            print('[TRAIN WWW] epoch %d/%d batch loss: %.4f (avg %.4f) (%.2f im/s)'\n",
        "                % (epoch + 1, num_epochs, loss_val, sum_epoch_loss / (i + 1),\n",
        "                  len(uids) / (time.time() - start)))\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "\n",
        "\n",
        "def validate(valid_loader, model):\n",
        "    \"\"\"\n",
        "    Auxiliary function for epoch validate/test\n",
        "\n",
        "    Running GraphRec model over given data loader (validation set loader / test set loader). \n",
        "\n",
        "    Returns mae and rmse score for given data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    with torch.no_grad():\n",
        "        for uids, iids, labels, u_items, u_users, u_users_items, i_users,u_user_group, u_user_item_group in tqdm(valid_loader):\n",
        "            uids = uids.to(device)\n",
        "            iids = iids.to(device)\n",
        "            labels = labels.to(device)\n",
        "            u_items = u_items.to(device)\n",
        "            u_users = u_users.to(device)\n",
        "            u_users_items = u_users_items.to(device)\n",
        "            i_users = i_users.to(device)\n",
        "            u_user_group =u_user_group.to(device)#### added \n",
        "            u_user_item_group = u_user_item_group.to(device)#### added \n",
        "\n",
        "            preds = model(uids, iids, u_items, u_users, u_users_items, i_users, u_user_group, u_user_item_group) #### changed\n",
        "            error = torch.abs(preds.squeeze(1) - labels)\n",
        "            errors.extend(error.data.cpu().numpy().tolist())\n",
        "    \n",
        "    mae = np.mean(errors)\n",
        "    rmse = np.sqrt(np.mean(np.power(errors, 2)))\n",
        "    return mae, rmse\n",
        "\n",
        "\n",
        "\n",
        "# RUN training/testing\n",
        "main(test = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
